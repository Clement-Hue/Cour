{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Traitement des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TP utilise la librairie python NTLK (natural language processing toolkit), qui est la librairie python de référence\n",
    "pour le traitement des textes. Elle est organisée de manière plus complexe que les autres librairies, puisqu'elle fait appel à de nombreux composants et corpus qu'on peut charger via un \"downloader\" spécifique, comme ci-dessous :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hors du notebook :\n",
    "\n",
    "installer le package python nltk (avec conda ou pip, par exemple)\n",
    "\n",
    "puis \n",
    "* `import nltk`\n",
    "* `nltk.download()`\n",
    "\n",
    "ce qui doit ouvrir une fenêtre pour chargement interactif de corpus et modules de traitement de la langue. Il faudra installer des modules au fur des besoins pour le TP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Ici s'agit ici de découper automatiquement un texte en phrases. Quelles sont les difficultés que doit résoudre la fonction (le travail est déjà fait !) ? (penser à la ponctuation et aux majuscules, modifier le texte pour confronter la fonction à des difficultés).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning is the lovely science of getting computers to act without being explicitly programmed.', 'In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome.', 'Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it.', 'Many researchers also think it is the best way to make progress towards human-level AI.', 'In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself.', \"More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems.\", \"Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\"]\n",
      "['Machine', 'learning', 'is', 'the', 'lovely', 'science', 'of', 'getting', 'computers', 'to', 'act', 'without', 'being', 'explicitly', 'programmed', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "# Ici un texte que vous êtes invités à modifier\n",
    "text = \"Machine learning is the lovely science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences) # Affichons le découpage en phrases\n",
    "\n",
    "premiere_phrase = sentences[0]\n",
    "les_mots=word_tokenize(premiere_phrase)\n",
    "print(les_mots) # Affichons le découpage en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également tenter d'identifier la nature grammaticale de chaque mot (Part-of-Speech) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'NN'),\n",
       " ('learning', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('lovely', 'JJ'),\n",
       " ('science', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('getting', 'VBG'),\n",
       " ('computers', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('act', 'VB'),\n",
       " ('without', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('explicitly', 'RB'),\n",
       " ('programmed', 'VBN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(les_mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "où VBP : verbe au présent, IN : préposition, NN: nom, NNP : nom propre, CC: conjonction de coordination, JJ : adjectif,...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'expressions composée de plusieurs mots successifs  (\"Machine learning\", \"pâté de campagne\",\"école d'ingénieurs\",\"abat-jour\",\"travaux pratiques) où dont sémantique est associée à la séquence et non à chaque mot pris isolément. \n",
    "\n",
    "* Proposer un principe statistique d'identification automatique des collocations dans un corpus de textes ? \n",
    "* Quelle est l'importance de cette identification pour, par exemple, la traduction ?\n",
    "\n",
    "Ci-dessous, un petit exemple sur le script (texte) de Monty Python-Sacré Graal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('Hello', 'Hello'), 267.112668959853),\n",
       " (('don', 't'), 261.26601625820626),\n",
       " (('clop', 'clop'), 260.48367234352037),\n",
       " (('mumble', 'mumble'), 210.87280109162216),\n",
       " (('Holy', 'Grail'), 203.6128877611493),\n",
       " (('squeak', 'squeak'), 200.11837146003575),\n",
       " (('saw', 'saw'), 199.43405994849405),\n",
       " (('ha', 'ha'), 195.5837918341728),\n",
       " (('Burn', 'her'), 189.491732138607),\n",
       " (('Sir', 'Robin'), 187.39540407668534),\n",
       " (('Ni', 'Ni'), 181.96030649440533),\n",
       " (('Run', 'away'), 176.98935205620185),\n",
       " (('witch', 'witch'), 172.35868495940355),\n",
       " (('Iesu', 'domine'), 157.49017281313883),\n",
       " (('Pie', 'Iesu'), 157.49017281313883),\n",
       " (('King', 'Arthur'), 151.01995532192427),\n",
       " (('going', 'to'), 145.03673312788362),\n",
       " (('Come', 'on'), 142.06822969964873),\n",
       " (('her', 'Burn'), 138.81153073577218),\n",
       " (('clang', 'Bring'), 134.52227490026843),\n",
       " (('Bring', 'out'), 129.873654900665),\n",
       " (('Round', 'Table'), 129.56408933134304),\n",
       " (('away', 'Run'), 121.21891004917532),\n",
       " (('clap', 'clap'), 119.30449709216805),\n",
       " (('the', 'Holy'), 119.02159869796324),\n",
       " (('He', 's'), 118.066516339802),\n",
       " (('All', 'right'), 117.61671343617802),\n",
       " (('dramatic', 'chord'), 115.23874132549803),\n",
       " (('Ha', 'ha'), 113.7893408027561),\n",
       " (('dona', 'eis'), 109.21041874539506),\n",
       " (('eis', 'requiem'), 109.21041874539506),\n",
       " (('didn', 't'), 107.98331570111827),\n",
       " (('of', 'the'), 106.34360554042851),\n",
       " (('What', 'is'), 105.12716673918058),\n",
       " (('Shut', 'up'), 104.14683481175071),\n",
       " (('Shh', 'Shh'), 103.26532644894652),\n",
       " (('No', 'no'), 101.33245970211681),\n",
       " (('That', 's'), 91.19661322503147),\n",
       " (('s', 'going'), 91.19661322503147),\n",
       " (('music', 'stops'), 89.48627829480021),\n",
       " (('It', 's'), 89.4176872215742),\n",
       " (('your', 'dead'), 88.47396827140847),\n",
       " (('doesn', 't'), 88.09563069741681),\n",
       " (('to', 'tell'), 86.87529038931069),\n",
       " (('Sir', 'Launcelot'), 83.30109095820518),\n",
       " (('Hold', 'it'), 81.85687300351778),\n",
       " (('ve', 'got'), 80.62941819113018),\n",
       " (('singing', 'He'), 80.35529677240476),\n",
       " (('dead', 'clang'), 77.24083565507446),\n",
       " (('Who', 'Say'), 73.072495987395),\n",
       " (('brave', 'Sir'), 72.28567785135058),\n",
       " (('Sir', 'Galahad'), 72.0509049345188),\n",
       " (('angels', 'sing'), 71.89704756560482),\n",
       " (('is', 'your'), 70.33046828762596),\n",
       " (('Hand', 'Grenade'), 70.3288760279156),\n",
       " (('single', 'handed'), 70.3288760279156),\n",
       " (('Thank', 'you'), 69.86548811035456),\n",
       " (('m', 'not'), 67.73086922328667),\n",
       " (('Arthur', 'music'), 67.33858957466732),\n",
       " (('out', 'your'), 66.8914102857281),\n",
       " (('chanting', 'Pie'), 66.41049873447548),\n",
       " (('we', 're'), 65.8069784830275),\n",
       " (('it', 's'), 65.50479313953403),\n",
       " (('questions', 'Three'), 64.87946587460057),\n",
       " (('Three', 'questions'), 64.87946587460056),\n",
       " (('my', 'liege'), 64.53723311194857),\n",
       " (('Oh', 'yeah'), 63.12997451324585),\n",
       " (('mumble', 'boom'), 62.107337707682575),\n",
       " (('Knights', 'Who'), 61.57513892053622),\n",
       " (('domine', 'dona'), 61.23429000359627),\n",
       " (('We', 'are'), 60.72681144215403),\n",
       " (('Arthur', 'King'), 58.72009729546264),\n",
       " (('isn', 't'), 58.482113596879785),\n",
       " (('rewr', 'rewr'), 57.95842720416803),\n",
       " (('have', 'seen'), 57.458652261846034),\n",
       " (('It', 'is'), 57.38299503549848),\n",
       " (('Launcelot', 'Launcelot'), 56.01053900948149),\n",
       " (('the', 'Round'), 55.84403860439439),\n",
       " (('in', 'the'), 55.70899106726049),\n",
       " (('Heh', 'heh'), 55.522801860979605),\n",
       " (('giggle', 'giggle'), 55.05336311870473),\n",
       " (('oral', 'sex'), 54.473059511804045),\n",
       " (('t', 'leave'), 53.96231102967991),\n",
       " (('boom', 'boom'), 53.83521481084068),\n",
       " (('heh', 'heh'), 53.08800458942175),\n",
       " (('make', 'sure'), 53.08800458942175),\n",
       " (('e', 'doesn'), 52.96057743303702),\n",
       " (('the', 'Britons'), 52.5450510818761),\n",
       " (('the', 'room'), 52.5450510818761),\n",
       " (('to', 'be'), 51.93689438368392),\n",
       " (('haw', 'haw'), 51.209310330950295),\n",
       " (('pound', 'pound'), 51.209310330950295),\n",
       " (('hee', 'hee'), 51.045907384530985),\n",
       " (('sure', 'e'), 50.050166440975936),\n",
       " (('Black', 'Beast'), 49.974378354853584),\n",
       " (('Brother', 'Maynard'), 49.974378354853584),\n",
       " (('Knights', 'of'), 49.91095995974132),\n",
       " (('You', 're'), 49.781279323701995),\n",
       " (('Bridge', 'of'), 49.607253923222586),\n",
       " (('of', 'Death'), 49.607253923222586),\n",
       " (('Holy', 'Hand'), 48.70193800233298),\n",
       " (('Say', 'Ni'), 48.425864042992174),\n",
       " (('Sir', 'Knight'), 48.322750301064225),\n",
       " (('Burn', 'Burn'), 48.127199966314556),\n",
       " (('Be', 'quiet'), 47.74294284171148),\n",
       " (('Jesus', 'Christ'), 47.74294284171148),\n",
       " (('Did', 'you'), 46.48839268907593),\n",
       " (('Of', 'course'), 46.15529334508471),\n",
       " (('seek', 'the'), 45.958223617416664),\n",
       " (('Quiet', 'Quiet'), 45.47590393761914),\n",
       " (('twenty', 'four'), 45.47590393761914),\n",
       " (('Stop', 'that'), 45.26419744785798),\n",
       " (('shut', 'up'), 45.2129651272173),\n",
       " (('Castle', 'Anthrax'), 44.91234604599745),\n",
       " (('English', 'k'), 44.91234604599745),\n",
       " (('My', 'liege'), 44.823714118426196),\n",
       " (('have', 'been'), 44.20200235929209),\n",
       " (('boom', 'mumble'), 43.923359309051136),\n",
       " (('a', 'witch'), 43.27877845269805),\n",
       " (('it', 'Hold'), 43.002319162414366),\n",
       " (('scene', 'twenty'), 41.65723247140051),\n",
       " (('music', 'music'), 41.59752001236155),\n",
       " (('of', 'Camelot'), 41.5713524178971),\n",
       " (('us', 'easily'), 41.21735887770634),\n",
       " (('the', 'Grail'), 41.10417203683214),\n",
       " (('j', 'j'), 41.013653215977946),\n",
       " (('Make', 'sure'), 40.977016040952655),\n",
       " (('King', 'of'), 40.77242806076879),\n",
       " (('that', 's'), 40.68206534670371),\n",
       " (('t', 'want'), 40.640473453552524),\n",
       " (('quest', 'for'), 40.33063672422352),\n",
       " (('s', 'not'), 39.95215048372803),\n",
       " (('sank', 'into'), 39.460986805658415),\n",
       " (('she', 'is'), 39.45830846743609),\n",
       " (('Oh', 'yes'), 39.005166589290894),\n",
       " (('no', 'no'), 38.779777246939375),\n",
       " (('Clear', 'off'), 38.252855992617086),\n",
       " (('stay', 'here'), 38.072813065900114),\n",
       " (('do', 'you'), 37.4578473718192),\n",
       " (('your', 'name'), 36.894242919719524),\n",
       " (('come', 'and'), 36.41338748439715),\n",
       " (('shalt', 'thou'), 36.28816945865032),\n",
       " (('Oh', 'shut'), 35.96263839430141),\n",
       " (('more', 'than'), 35.93113901220296),\n",
       " (('court', 'at'), 35.63612633628033),\n",
       " (('by', 'surprise'), 35.428418172304326),\n",
       " (('stops', 'Aaagh'), 35.428418172304326),\n",
       " (('spanking', 'spanking'), 35.35494144183092),\n",
       " (('you', 'Thank'), 35.15526056394003),\n",
       " (('can', 't'), 35.067912735895064),\n",
       " (('We', 're'), 34.90777273368636),\n",
       " (('get', 'him'), 34.88090471561036),\n",
       " (('does', 'it'), 34.7612081285374),\n",
       " (('said', 'it'), 34.7612081285374),\n",
       " (('The', 'Tale'), 34.5607936038457),\n",
       " (('Well', 'what'), 34.55767468614049),\n",
       " (('Sir', 'Bedevere'), 34.50690968820173),\n",
       " (('looking', 'for'), 34.36707708675433),\n",
       " (('let', 's'), 34.146231455751554),\n",
       " (('are', 'you'), 33.49487061297991),\n",
       " (('my', 'court'), 33.48254940886635),\n",
       " (('Let', 'him'), 33.364316946473394),\n",
       " (('Are', 'you'), 33.35072685677477),\n",
       " (('To', 'seek'), 32.70085141120227),\n",
       " (('re', 'not'), 32.69468194231193),\n",
       " (('Hang', 'on'), 32.15954646787418),\n",
       " (('is', 'a'), 32.01477822761639),\n",
       " (('your', 'quest'), 31.732654652502227),\n",
       " (('Please', 'please'), 31.678001292469247),\n",
       " (('requiem', 'Pie'), 31.678001292469247),\n",
       " (('five', 'questions'), 31.613884494823065),\n",
       " (('Yes', 'Let'), 31.58559083261799),\n",
       " (('it', 'again'), 31.5256971872408),\n",
       " (('got', 'a'), 31.296714911011854),\n",
       " (('We', 'have'), 31.14467044304134),\n",
       " (('already', 'got'), 31.14241284036022),\n",
       " (('quest', 'To'), 30.87531306547206),\n",
       " (('shrubbery', 'dramatic'), 30.87531306547206),\n",
       " (('We', 'haven'), 30.651230310299347),\n",
       " (('not', 'quite'), 30.144690641393122),\n",
       " (('Get', 'back'), 30.04862396338502),\n",
       " (('a', 'shrubbery'), 29.8050827343401),\n",
       " (('singing', 'stops'), 29.791593238003493),\n",
       " (('keep', 'him'), 29.687161980502264),\n",
       " (('i', 'if'), 29.66666760757351),\n",
       " (('from', 'the'), 29.657358231525137),\n",
       " (('tell', 'He'), 29.64946759138096),\n",
       " (('Nu', 'No'), 29.20034392795549),\n",
       " (('Don', 't'), 29.12033207256252),\n",
       " (('haven', 't'), 29.12033207256252),\n",
       " (('sure', 'he'), 28.502149280152118),\n",
       " (('We', 've'), 28.38872112241816),\n",
       " (('Heh', 'hee'), 28.224948778849676),\n",
       " (('you', 're'), 28.015819465958742),\n",
       " (('hee', 'ha'), 27.934122360627228),\n",
       " (('Who', 'are'), 27.93295982052448),\n",
       " (('chops', 'the'), 27.869434269737955),\n",
       " (('with', 'it'), 27.869020587329707),\n",
       " (('rescue', 'me'), 27.80316272166809),\n",
       " (('and', 'get'), 27.476680219340622),\n",
       " (('kill', 'him'), 27.46214602143459),\n",
       " (('quite', 'dead'), 27.402562334485165),\n",
       " (('no', 'No'), 27.277209203204045),\n",
       " (('if', 'if'), 27.157867241329367),\n",
       " (('and', 'make'), 26.79714781577955),\n",
       " (('my', 'nose'), 26.76693076121817),\n",
       " (('seen', 'it'), 26.691553552622945),\n",
       " (('ve', 'found'), 26.681789130498455),\n",
       " (('If', 'you'), 26.51690933454462),\n",
       " (('no', 'man'), 26.467063049675136),\n",
       " (('us', 'where'), 26.26104173800052),\n",
       " (('and', 'his'), 26.183129932101508),\n",
       " (('you', 'do'), 26.028970229360944),\n",
       " (('Not', 'like'), 25.987293176325018),\n",
       " (('a', 'bit'), 25.87138087641194),\n",
       " (('howl', 'boom'), 25.716644373591286),\n",
       " (('for', 'the'), 25.64495217306883),\n",
       " (('the', 'Knights'), 25.458380205658713),\n",
       " (('talk', 'to'), 25.308792052567984),\n",
       " (('This', 'is'), 25.259057250150775),\n",
       " (('Brave', 'Sir'), 24.823971033073665),\n",
       " (('Tale', 'of'), 24.746489052257672),\n",
       " (('sort', 'of'), 24.746489052257672),\n",
       " (('You', 'are'), 24.740328302109674),\n",
       " (('tell', 'us'), 24.704709981060105),\n",
       " (('in', 'this'), 24.584529788206915),\n",
       " (('on', 'with'), 24.518912164458207),\n",
       " (('Hic', 'Right'), 24.20868498786602),\n",
       " (('Go', 'away'), 23.950834991329643),\n",
       " (('on', 'Back'), 23.869133609901986),\n",
       " (('right', 'All'), 23.785665850310632),\n",
       " (('got', 'one'), 23.69697617455011),\n",
       " (('who', 'had'), 23.240129678955192),\n",
       " (('Will', 'you'), 23.20034879512088),\n",
       " (('tell', 'Shut'), 22.931178753666284),\n",
       " (('they', 'were'), 22.850742413030716),\n",
       " (('join', 'me'), 22.767863438287463),\n",
       " (('your', 'master'), 22.58120253886107),\n",
       " (('Oh', 'wicked'), 22.467985426750694),\n",
       " (('They', 're'), 22.462724115701626),\n",
       " (('You', 'know'), 22.40399936631397),\n",
       " (('he', 's'), 22.351641756468645),\n",
       " (('into', 'the'), 22.207125183008934),\n",
       " (('where', 'we'), 22.168428587842413),\n",
       " (('does', 'he'), 22.168307914424656),\n",
       " (('a', 'lot'), 22.167935839538625),\n",
       " (('How', 'do'), 22.007084394937813),\n",
       " (('And', 'his'), 21.761558052023823),\n",
       " (('take', 'him'), 21.339836378051086),\n",
       " (('name', 'of'), 21.093267442423887),\n",
       " (('like', 'that'), 20.95160686145345),\n",
       " (('have', 'found'), 20.882906154905104),\n",
       " (('you', 'must'), 20.789904197491218),\n",
       " (('We', 'shall'), 20.684501230441914),\n",
       " (('found', 'a'), 20.67671497465271),\n",
       " (('You', 've'), 20.551230789138746),\n",
       " (('t', 'know'), 20.527131772705445),\n",
       " (('we', 'shall'), 20.420261881601622),\n",
       " (('that', 'Stop'), 20.370965167494827),\n",
       " (('the', 'Bridge'), 20.35515216479065),\n",
       " (('made', 'of'), 20.280110358492717),\n",
       " (('Oh', 'thank'), 20.258795465215755),\n",
       " (('if', 'you'), 20.241627444224182),\n",
       " (('In', 'the'), 20.065465843078208),\n",
       " (('find', 'the'), 20.065465843078208),\n",
       " (('Get', 'on'), 20.006098044720105),\n",
       " (('Oh', 'don'), 20.001718170856275),\n",
       " (('God', 'be'), 19.88723649090084),\n",
       " (('You', 'must'), 19.77154898093776),\n",
       " (('peril', 'No'), 19.70023366883299),\n",
       " (('here', 'in'), 19.620033937432495),\n",
       " (('to', 'find'), 19.556439604302867),\n",
       " (('a', 'g'), 19.234144609415303),\n",
       " (('g', 'a'), 19.234144609415303),\n",
       " (('not', 'afraid'), 19.188708111383654),\n",
       " (('out', 'of'), 19.170639228992734),\n",
       " (('at', 'the'), 19.164921588840315),\n",
       " (('Knight', 'of'), 18.824743331964793),\n",
       " (('the', 'last'), 18.494562433178107),\n",
       " (('the', 'word'), 18.494562433178107),\n",
       " (('Stop', 'it'), 18.45036796166416),\n",
       " (('Camelot', 'What'), 18.22934119229027),\n",
       " (('there', 'was'), 18.137558866512627),\n",
       " (('of', 'wood'), 18.080980674925343),\n",
       " (('want', 'to'), 17.987111301533126),\n",
       " (('this', 'land'), 17.956664031032943),\n",
       " (('all', 'right'), 17.84389513574518),\n",
       " (('No', 'No'), 17.685081724564895),\n",
       " (('Ni', 'Shh'), 17.25693804770541),\n",
       " (('no', 'more'), 17.249961240458163),\n",
       " (('s', 'got'), 17.21415187015056),\n",
       " (('s', 'what'), 17.21415187015056),\n",
       " (('your', 'king'), 17.16876495124608),\n",
       " (('at', 'Camelot'), 17.15390050180296),\n",
       " (('a', 'duck'), 17.041328394553975),\n",
       " (('the', 'rabbit'), 17.026881231422585),\n",
       " (('could', 'be'), 16.983991308745193),\n",
       " (('There', 's'), 16.860224060752806),\n",
       " (('you', 'get'), 16.846586755360924),\n",
       " (('you', 'mean'), 16.82519737937072),\n",
       " (('this', 'is'), 16.819890649268743),\n",
       " (('thank', 'you'), 16.554217035362562),\n",
       " (('of', 'course'), 16.52564037496094),\n",
       " (('the', 'most'), 16.455739604085604),\n",
       " (('the', 'number'), 16.455739604085604),\n",
       " (('the', 'same'), 16.455739604085604),\n",
       " (('am', 'Arthur'), 16.425317547056277),\n",
       " (('in', 'which'), 16.34791848345843),\n",
       " (('ll', 'be'), 16.270395215025534),\n",
       " (('at', 'all'), 16.265455566730576),\n",
       " (('but', 'don'), 16.102434129679338),\n",
       " (('Grail', 'What'), 16.02530246756278),\n",
       " (('What', 'do'), 16.02530246756278),\n",
       " (('She', 's'), 15.990701877595438),\n",
       " (('witch', 'We'), 15.973023231435587),\n",
       " (('to', 'join'), 15.865550108170316),\n",
       " (('you', 'go'), 15.841045801956522),\n",
       " (('the', 'Lord'), 15.813894471124112),\n",
       " (('with', 'a'), 15.553336375925708),\n",
       " (('what', 'do'), 15.496728133645007),\n",
       " (('a', 'coconut'), 15.492302227705608),\n",
       " (('a', 'second'), 15.492302227705608),\n",
       " (('do', 'not'), 15.445392866912325),\n",
       " (('it', 'Yes'), 15.406633721406724),\n",
       " (('here', 'and'), 15.299271825016941),\n",
       " (('he', 'is'), 15.265795949465103),\n",
       " (('us', 'all'), 15.218145686682067),\n",
       " (('the', 'name'), 14.9972886624923),\n",
       " (('re', 'we'), 14.897181841584885),\n",
       " (('if', 'he'), 14.801104746463539),\n",
       " (('this', 'castle'), 14.70878412991053),\n",
       " (('He', 'was'), 14.39525347479201),\n",
       " (('a', 'silly'), 14.287982290729694),\n",
       " (('the', 'door'), 14.286458061325051),\n",
       " (('the', 'forest'), 14.286458061325051),\n",
       " (('the', 'other'), 14.286458061325051),\n",
       " (('We', 'll'), 14.266303605120525),\n",
       " (('music', 'clop'), 14.223934655086978),\n",
       " (('s', 'only'), 14.151655315220708),\n",
       " (('witch', 'Burn'), 13.941547911048556),\n",
       " (('him', 'in'), 13.85467900161128),\n",
       " (('at', 'you'), 13.75663449264272),\n",
       " (('No', 'not'), 13.73895286809976),\n",
       " (('it', 'Oh'), 13.57409614955634),\n",
       " (('a', 'very'), 13.562820004206257),\n",
       " (('a', 'cave'), 13.302315341297774),\n",
       " (('were', 'in'), 13.226219732637784),\n",
       " (('we', 'will'), 13.156644831267412),\n",
       " (('Galahad', 'and'), 13.029181975422393),\n",
       " (('in', 'my'), 12.979645942825499),\n",
       " (('you', 'could'), 12.946531072680717),\n",
       " (('you', 'know'), 12.946531072680717),\n",
       " (('you', 'see'), 12.946531072680717),\n",
       " (('t', 'be'), 12.857246321667121),\n",
       " (('it', 'say'), 12.802336200987794),\n",
       " (('t', 'think'), 12.792063209183105),\n",
       " (('the', 'swamp'), 12.76096906182419),\n",
       " (('shall', 'not'), 12.72972835353809),\n",
       " (('What', 'are'), 12.68056921559895),\n",
       " (('mean', 'to'), 12.652826448590492),\n",
       " (('if', 'we'), 12.621708382092796),\n",
       " (('Oh', 'no'), 12.583820531650051),\n",
       " (('Who', 's'), 12.559919302083975),\n",
       " (('Oh', 'but'), 12.467237573676982),\n",
       " (('not', 'dead'), 12.443025741908233),\n",
       " (('There', 'is'), 12.414778104403617),\n",
       " (('you', 'can'), 12.10245662693613),\n",
       " (('am', 'your'), 12.04005266184601),\n",
       " (('you', 'think'), 12.023611362281585),\n",
       " (('was', 'not'), 11.911425406364657),\n",
       " (('have', 'a'), 11.837972998820263),\n",
       " (('a', 'king'), 11.747316004200336),\n",
       " (('Well', 'can'), 11.695411501732146),\n",
       " (('name', 'is'), 11.625746262229402),\n",
       " (('the', 'Castle'), 11.580188787977688),\n",
       " (('the', 'old'), 11.580188787977688),\n",
       " (('up', 'And'), 11.446068654360015),\n",
       " (('Oh', 'see'), 11.41206454738062),\n",
       " (('did', 'you'), 11.27708673856779),\n",
       " (('this', 'one'), 11.17963132033082),\n",
       " (('a', 'Knight'), 11.112287942491493),\n",
       " (('a', 'good'), 11.112287942491493),\n",
       " (('found', 'the'), 10.618063997985143),\n",
       " (('the', 'cave'), 10.618063997985143),\n",
       " (('have', 'to'), 10.522780254590224),\n",
       " (('to', 'have'), 10.522780254590224),\n",
       " (('Well', 'that'), 10.481208164548892),\n",
       " (('in', 'Camelot'), 10.244728521786026),\n",
       " (('see', 'the'), 10.124955626698192),\n",
       " (('d', 'you'), 10.082375677113047),\n",
       " (('when', 'you'), 10.082375677113047),\n",
       " (('Arthur', 'and'), 10.054859069627225),\n",
       " (('That', 'is'), 10.051422511065002),\n",
       " (('me', 'And'), 10.015914007352173),\n",
       " (('No', 'it'), 9.916954954207815),\n",
       " (('one', 'of'), 9.807258665483253),\n",
       " (('come', 'to'), 9.529359290228946),\n",
       " (('on', 'a'), 9.295945643824048),\n",
       " (('as', 'a'), 9.145787212420926),\n",
       " (('into', 'a'), 9.145787212420926),\n",
       " (('is', 'that'), 9.133961285315232),\n",
       " (('with', 'your'), 8.81475711701773),\n",
       " (('what', 'it'), 8.795146701670284),\n",
       " (('go', 'and'), 8.697273239982255),\n",
       " (('in', 'here'), 8.685946171812176),\n",
       " (('have', 'no'), 8.670779445848165),\n",
       " (('Right', 'Oh'), 8.641967065217266),\n",
       " (('And', 'this'), 8.604948942795215),\n",
       " (('leave', 'the'), 8.49867806066289),\n",
       " (('the', 'land'), 8.49867806066289),\n",
       " (('you', 'sir'), 8.304082534082195),\n",
       " (('are', 'We'), 8.292494062423158),\n",
       " (('we', 'are'), 8.111184305381148),\n",
       " (('but', 'you'), 8.011521413908063),\n",
       " (('we', 'have'), 8.005796358374766),\n",
       " (('you', 'are'), 7.753609844696866),\n",
       " (('there', 's'), 7.723052482000468),\n",
       " (('for', 'it'), 7.656152918656417),\n",
       " (('s', 'it'), 7.625970513331318),\n",
       " (('you', 'come'), 7.610460218752139),\n",
       " (('just', 'to'), 7.609242275510595),\n",
       " (('you', 'have'), 7.595218137117727),\n",
       " (('is', 'here'), 7.540843000271807),\n",
       " (('get', 'a'), 7.442855277671768),\n",
       " (('to', 'Camelot'), 7.388310239466655),\n",
       " (('Grail', 'Oh'), 7.341405856036535),\n",
       " (('here', 'Oh'), 7.341405856036535),\n",
       " (('it', 'It'), 7.329912450230973),\n",
       " (('think', 'you'), 7.298551531188711),\n",
       " (('what', 's'), 7.289971869329775),\n",
       " (('Launcelot', 'of'), 7.111672396837724),\n",
       " (('of', 'Ni'), 7.0434375929919035),\n",
       " (('the', 'questions'), 7.028018837307539),\n",
       " (('am', 'a'), 6.903077634134507),\n",
       " (('re', 'a'), 6.822574881883527),\n",
       " (('with', 'you'), 6.802636613226939),\n",
       " (('t', 'Well'), 6.7331766018215795),\n",
       " (('you', 'shall'), 6.73196004569793),\n",
       " (('you', 'going'), 6.4734017683092056),\n",
       " (('you', 'will'), 6.4734017683092056),\n",
       " (('Yes', 'Oh'), 6.1988616463059785),\n",
       " (('just', 'a'), 6.198429201328274),\n",
       " (('not', 'a'), 6.010112318015202),\n",
       " (('all', 'the'), 5.921541932440203),\n",
       " (('He', 'is'), 5.912218602394215),\n",
       " (('the', 'castle'), 5.604967260412333),\n",
       " (('is', 'the'), 5.139480946434459),\n",
       " (('Well', 'it'), 5.045582183366707),\n",
       " (('s', 'a'), 4.847847737875474),\n",
       " (('of', 'her'), 4.633170610774149),\n",
       " (('Well', 'you'), 4.530139960692096),\n",
       " (('can', 'you'), 4.507584814633854),\n",
       " (('to', 'me'), 4.394057961262979),\n",
       " (('is', 'it'), 4.319061925825482),\n",
       " (('it', 'is'), 4.319061925825482),\n",
       " (('and', 'Sir'), 4.110670286016639),\n",
       " (('s', 'that'), 4.0727309285296505),\n",
       " (('Oh', 'Oh'), 3.9451110143538735),\n",
       " (('not', 'is'), 3.84019490831864),\n",
       " (('be', 'a'), 3.480793562000426),\n",
       " (('of', 'Sir'), 3.3939978116592506),\n",
       " (('and', 'we'), 3.302681991900389),\n",
       " (('And', 'the'), 2.848521985893636),\n",
       " (('are', 'the'), 2.646253553408572),\n",
       " (('And', 'you'), 2.4818957295026154),\n",
       " (('in', 'a'), 2.4208768775761538),\n",
       " (('to', 'your'), 2.2391809951006723),\n",
       " (('with', 'the'), 2.080815378800815),\n",
       " (('of', 'your'), 1.8892908105059414),\n",
       " (('and', 'the'), 1.68056191520333),\n",
       " (('be', 'the'), 1.6220946001544383),\n",
       " (('of', 'that'), 1.4891704937278951),\n",
       " (('me', 'the'), 1.3903980361704082),\n",
       " (('to', 'the'), 1.3131884713235413),\n",
       " (('it', 'and'), 1.2128028873221592),\n",
       " (('of', 'a'), 1.0633631987706358),\n",
       " (('that', 'you'), 0.7310660063907815),\n",
       " (('s', 'the'), 0.49430048666377857),\n",
       " (('Oh', 'you'), 0.18973074462510642),\n",
       " (('you', 'Oh'), 0.18973074462510642),\n",
       " (('a', 'a'), 0.12867403568155966),\n",
       " (('the', 's'), 0.03132047505019342),\n",
       " (('to', 'you'), 0.0004282203278771768),\n",
       " (('you', 'a'), 0.00036266353196598433)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.book import *\n",
    "import re\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "montypython_words = sum([re.findall(r'[A-Z]?[a-z]+', token) for token in text6.tokens], [])\n",
    "finder = BigramCollocationFinder.from_words(montypython_words)\n",
    "finder.apply_freq_filter(3)\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "finder.score_ngrams(bigram_measures.likelihood_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'extraire automatiquement la racine de mots. \n",
    "Le premier des algorithmes est expliqué ici :\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\n",
    "Quel est l'intérêt de cette opération, dans la perspective de calculer les fréquences d'occurrence des termes dans un texte ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comput\n",
      "comput\n",
      "work\n",
      "work\n",
      "come\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem(\"computer\"))\n",
    "print(porter_stemmer.stem(\"computation\"))\n",
    "print(porter_stemmer.stem(\"working\"))\n",
    "print(porter_stemmer.stem(\"worked\"))\n",
    "print(porter_stemmer.stem(\"coming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n",
      "fin\n",
      "fin\n",
      "fin\n",
      "infin\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"french\")\n",
    "print(snowball_stemmer.stem(\"finir\"))\n",
    "print(snowball_stemmer.stem(\"finis\"))\n",
    "print(snowball_stemmer.stem(\"finissant\"))\n",
    "print(snowball_stemmer.stem(\"fini\"))\n",
    "print(snowball_stemmer.stem(\"infini\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation\n",
    "\n",
    "Il s'agit d'un objectif voisin, mais plus soigné parce qu'exploitant des ressources linguistiques (ici la base Wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize('was', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List des mots à enlever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fut', 'aux', 'eut', 'serez', 'mes', 'ma', 'ayons', 'on', 'des', 'aura', 'ayants', 'une', 'c', 'en', 'te', 'es', 'ses', 'étées', 'vos', 'le', 'ton', 'nos', 'auraient', 'fusse', 'eût', 'toi', 'ce', 'sont', 'sa', 'seriez', 'fussions', 'tu', 'soient', 'seraient', 'sommes', 'eûtes', 'aies', 'ne', 'mais', 'ta', 'eusse', 'seront', 't', 'aie', 'du', 'leur', 'de', 'avaient', 'fusses', 'eussions', 'ayantes', 'suis', 'l', 'tes', 'eue', 'aviez', 'eus', 'avez', 'avec', 'auras', 'n', 'ayante', 'son', 'pour', 'j', 'd', 'la', 'ait', 'nous', 'qu', 'dans', 'fussent', 'sois', 'serait', 'à', 'eues', 'étant', 'vous', 'notre', 'soyez', 's', 'étants', 'fussiez', 'serai', 'fût', 'moi', 'auriez', 'je', 'fûtes', 'fus', 'aurai', 'étiez', 'même', 'm', 'êtes', 'et', 'sur', 'étée', 'eux', 'eurent', 'furent', 'étions', 'auront', 'étantes', 'il', 'étés', 'aurez', 'serions', 'est', 'ai', 'as', 'eûmes', 'un', 'me', 'été', 'aurais', 'eusses', 'ayant', 'serons', 'ou', 'se', 'y', 'était', 'eussent', 'ont', 'elle', 'qui', 'serais', 'étais', 'votre', 'fûmes', 'eu', 'ces', 'aurions', 'avait', 'soit', 'les', 'avons', 'au', 'ils', 'avais', 'seras', 'eussiez', 'pas', 'lui', 'étante', 'ayez', 'sera', 'étaient', 'par', 'avions', 'aurait', 'aient', 'que', 'mon', 'aurons', 'soyons'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('french'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Découpage et suppression des \"stop words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['voici', 'longue', 'phrase', 'contient', 'plusieurs', 'petits', 'longs', 'mots']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Voici une longue phrase qui contient plusieurs petits et longs mots\"\n",
    "\n",
    "mots_non_stop = [i for i in sentence.lower().split() if i not in stop]\n",
    "print(mots_non_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ressource wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('computer.n.01'), Synset('calculator.n.01')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "wn.synsets('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a machine for performing calculations automatically'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('computer.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('analog_computer.n.01'),\n",
       " Synset('digital_computer.n.01'),\n",
       " Synset('home_computer.n.01'),\n",
       " Synset('node.n.08'),\n",
       " Synset('number_cruncher.n.02'),\n",
       " Synset('pari-mutuel_machine.n.01'),\n",
       " Synset('predictor.n.03'),\n",
       " Synset('server.n.03'),\n",
       " Synset('turing_machine.n.01'),\n",
       " Synset('web_site.n.01')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('computer.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('nasty.a.01.nasty')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('nice.a.01').lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('ugly.a.01.ugly')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dark')\n",
    "wn.synset('beautiful.a.01').lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordnet pour l'analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<failure.n.03: PosScore=0.125 NegScore=0.375>\n",
      "<fantastic.s.02: PosScore=0.75 NegScore=0.0>\n",
      "<hate.v.01: PosScore=0.0 NegScore=0.75>\n",
      "<generator object SentiWordNetCorpusReader.all_senti_synsets at 0x7f075145e9e8>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "# permet de savoir si on dit du bien, ou du mal d'un texte\n",
    "feeling = swn.senti_synset('failure.n.03')\n",
    "print(feeling)\n",
    "feeling = swn.senti_synset('wonderful.a.01')\n",
    "print(feeling)\n",
    "feeling = swn.senti_synset('hate.v.01')\n",
    "print(feeling)\n",
    "all = swn.all_senti_synsets()\n",
    "print(all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification de thèmes par factorisation matricielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapté de : \n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\", \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\", \"Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to continue this think tank charade of yours, your\\nfixation on Israel must stop.  You might have to start asking the\\nsame sort of questions of Arab countries as well.  You realize it\\nwould not work, as the Arab countries' treatment of Jews over the\\nlast several decades is so bad that your fixation on Israel would\\nbegin to look like the biased attack that it is.\\n\\n   Everyone in this group recognizes that your stupid 'Center for\\nPolicy Research' is nothing more than a fancy name for some bigot\\nwho hates Israel.\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_samples = dataset.data[:n_samples]\n",
    "\n",
    "print(data_samples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fabrication d'une matrice tf.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "  (0, 708)\t0.12621877625178227\n",
      "  (0, 410)\t0.11650651629173196\n",
      "  (0, 493)\t0.1631127602376565\n",
      "  (0, 548)\t0.11873384536901997\n",
      "  (0, 130)\t0.13595955391213657\n",
      "  (0, 567)\t0.13595955391213657\n",
      "  (0, 412)\t0.12831668397369733\n",
      "  (0, 750)\t0.15376128408643466\n",
      "  (0, 841)\t0.18564440175793037\n",
      "  (0, 206)\t0.15810189392327795\n",
      "  (0, 764)\t0.1640284908630232\n",
      "  (0, 748)\t0.13595955391213657\n",
      "  (0, 904)\t0.08983671288492111\n",
      "  (0, 923)\t0.11966934266418663\n",
      "  (0, 527)\t0.1690393571774018\n",
      "  (0, 432)\t0.13369075280946802\n",
      "  (0, 988)\t0.12740095334833063\n",
      "  (0, 488)\t0.3750048191807266\n",
      "  (0, 717)\t0.17767638066823058\n",
      "  (0, 587)\t0.6454209423982519\n",
      "  (0, 862)\t0.1551447391479567\n",
      "  (0, 286)\t0.11115911128919416\n",
      "  (0, 867)\t0.15810189392327795\n",
      "  (0, 881)\t0.11227372176926384\n",
      "  (1, 381)\t0.20157910011124136\n",
      "  :\t:\n",
      "  (1998, 504)\t0.04875543232365812\n",
      "  (1998, 991)\t0.053978162418983656\n",
      "  (1998, 566)\t0.03637572081429063\n",
      "  (1998, 611)\t0.05504978412016225\n",
      "  (1998, 171)\t0.047384737904817335\n",
      "  (1998, 414)\t0.08876861152823663\n",
      "  (1998, 268)\t0.23575826480007847\n",
      "  (1998, 491)\t0.1114848475886964\n",
      "  (1998, 271)\t0.05622767285588837\n",
      "  (1998, 907)\t0.06818500433590943\n",
      "  (1998, 710)\t0.05998220148907317\n",
      "  (1998, 998)\t0.04605022195294345\n",
      "  (1998, 173)\t0.10248793661244614\n",
      "  (1998, 122)\t0.05810140044184461\n",
      "  (1998, 984)\t0.0397488592737133\n",
      "  (1998, 533)\t0.05951387738098097\n",
      "  (1998, 306)\t0.030847223209189208\n",
      "  (1998, 540)\t0.12852849537452227\n",
      "  (1998, 130)\t0.04971790762820881\n",
      "  (1998, 750)\t0.05622767285588837\n",
      "  (1998, 286)\t0.04064884201283483\n",
      "  (1999, 738)\t0.5707845186348437\n",
      "  (1999, 366)\t0.56500361648845\n",
      "  (1999, 356)\t0.44578463121221495\n",
      "  (1999, 286)\t0.3952872489933768\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "# tfidf est une matrice creuse\n",
    "\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorisation de la matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with tf-idf features, n_samples=2000 and n_features=1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clement/.local/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "just people don think like know time good make way really say right ve want did ll new use years\n",
      "Topic #1:\n",
      "windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
      "Topic #2:\n",
      "god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
      "Topic #3:\n",
      "thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
      "Topic #4:\n",
      "car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
      "Topic #5:\n",
      "edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
      "Topic #6:\n",
      "file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
      "Topic #7:\n",
      "game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
      "Topic #8:\n",
      "drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
      "Topic #9:\n",
      "key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
